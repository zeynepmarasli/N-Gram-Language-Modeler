{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Name: Zeynep Marasli \n",
    "\n",
    "Algorithm Outline:\n",
    "1. Preprocess text:\n",
    "    - all lowercase; remove numerical values\n",
    "    - tokenize (word)\n",
    "    - add beginning/end sentence markers <s>, </s>\n",
    "2. Identify rare tokens in trainining set based on freq threshold \n",
    "3. Replace all rare tokens in training set with <UNK>\n",
    "4. Create new vocabulary V’: all tokens in modified training set (V’ = {all \n",
    "   common words, <UNK>, <s>, </s>})\n",
    "5. Generate unigram matrix frequency table\n",
    "6. Generate bigram matrix table (all possible bigrams from V') \n",
    "7. Count frequencies of occurring bigrams \n",
    "8. GT Smoothing:\n",
    "    - generate a frequency of frequency table that keeps track of the count frequencies in the dataset\n",
    "    - for bigrams with a count frequency between 0 and 10, GT probabilities are calculated. (count freq of 1 is treated as 0)\n",
    "    - for bigrams with a count frequency greater than 10, MLE is calculated. \n",
    "9. In test string, replace all unseen tokens with <UNK> \n",
    "10. Compute probability/perplexity of test string  \n",
    "11. Return perplexities of each test string as list\n",
    "12. Compare each of the 3 language models' perpelxities for each test string \n",
    "    ex. If English model has lowest perplexity of a sentence, then decide test string is English, etc. \n",
    "13. Write test results to text file\n",
    "14. Assess accuracy of model --> % correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import math\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Bigram_Model():\n",
    "    def __init__(self):\n",
    "        self.tokens = [] #the preprocssed/tokenized/UNK treated training set  \n",
    "        self.bigram_freq = {}\n",
    "        self.unigram_freq = {}\n",
    "        self.bigram_prob = {}\n",
    "    \n",
    "    def load_input(self, file):\n",
    "        f = open(file, \"r\")\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "        return text \n",
    "    \n",
    "    # helper function: called in preprocess()\n",
    "    # takes in a string & returns if tokenized by character\n",
    "    # adds beginning/end sentence markers \n",
    "    def tokenize(self, string):\n",
    "        tokens_list = [] \n",
    "        lines = str.splitlines(string)\n",
    "        for line in lines:\n",
    "            tokens_list.append(\"<s>\")\n",
    "            tokens_list = tokens_list + line.split() #split line by whitespaces \n",
    "            tokens_list.append(\"</s>\")\n",
    "        return tokens_list \n",
    "    \n",
    "    # helper function: called in UNK_treated_input()\n",
    "    # replaces a given item in tokens_list with UNK based on frequency \n",
    "    def insert_UNK(self, item_to_replace, tokens_list):\n",
    "        for index, token in enumerate(tokens_list):\n",
    "            if token == item_to_replace: tokens_list[index] = \"<UNK>\"\n",
    "        return tokens_list\n",
    "       \n",
    "\n",
    "    # helper function: called in preprocess()\n",
    "    # returns UNK modified training set --> all rare tokens are replaced with <UNK>\n",
    "    def UNK_treated_input(self, tokens_list, UNK_threshold = 5):\n",
    "        freq_dict = self.calc_ngram_freq(tokens_list, keys = tokens_list)\n",
    "        for key, value in freq_dict.items(): #replace rare terms in training set based on frequency\n",
    "            if value <= UNK_threshold: tokens_list = self.insert_UNK(key, tokens_list)\n",
    "        return tokens_list\n",
    "       \n",
    "    # will perform various regex sub steps (ex. remove all numeric characters)\n",
    "    # will tokenize text & add beginning & end sentence markers \n",
    "    # will replace low frequency words with UNK token \n",
    "    def preprocess_training(self, string, UNK_threshold = 5):\n",
    "        string = self.clean_string(string)\n",
    "        tokens_list = self.tokenize(string)\n",
    "        tokens_list = self.UNK_treated_input(tokens_list, UNK_threshold)\n",
    "        return tokens_list\n",
    "    \n",
    "    # called in preprocess_training()\n",
    "    # performs a series of regex substitutions to clean up raw text string \n",
    "    def clean_string(self, text_string):\n",
    "        text_string = text_string.lower()\n",
    "        text_string = re.sub(r\"\\d+\", \"\", text_string) #remove numerical characters\n",
    "        text_string = re.sub(r\"[$&%#-\\*]\", \"\", text_string)\n",
    "        text_string = re.sub(' +', ' ', text_string) #remove multiple spaces\n",
    "        return text_string\n",
    "       \n",
    "    def generate_ngrams(self, n, tokens):\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        ngrams = list(ngrams)\n",
    "        for ngram in ngrams: ngram = ''.join(ngram)\n",
    "        #print(ngrams)\n",
    "        return ngrams \n",
    "    \n",
    "    # given UNK_treated vocabulary, will generate all possible bigrams & return as list \n",
    "    def generate_bigram_matrix(self):\n",
    "        bigram_matrix = []\n",
    "        vocabulary = self.get_vocabulary()\n",
    "        for token_1 in vocabulary:\n",
    "            for token_2 in vocabulary:\n",
    "                bigram = (token_1, token_2)\n",
    "                bigram_matrix.append(bigram)\n",
    "        return bigram_matrix\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return list(self.unigram_freq.keys())\n",
    "    \n",
    "    # helper function: called in train() & UNK_treated_input()\n",
    "    # creates a freq dict from a list of ngrams (keys) and a list of ngrams occurring in the text (ngram_list)\n",
    "    # called in \n",
    "    def calc_ngram_freq(self, ngram_list, keys = []):\n",
    "        freq_dict = {}\n",
    "        for key in keys:\n",
    "            freq_dict[key] = 0 # initialize dict & set all values to 0\n",
    "        for item in ngram_list:\n",
    "            if item in freq_dict: freq_dict[item] = freq_dict[item] + 1\n",
    "            else: freq_dict[item] = 1 # create key if not in list [error catching]\n",
    "        return freq_dict\n",
    "    \n",
    "    # helper function: called in calc_bigram_prob()\n",
    "    def calc_mle(self, num, denom):\n",
    "        if denom == 0:\n",
    "            print(\"DNE, returning 0\")\n",
    "            return 0\n",
    "        return num/denom\n",
    "        \n",
    "    # used to calculate perplexity\n",
    "    # returns the number of tokens in a given list, exluding beginning of sentence markers \n",
    "    def token_count(self, tokens_list):\n",
    "        count = 0 \n",
    "        for token in tokens_list:\n",
    "            if token == \"<s>\": continue #do not count \n",
    "            else: count = count + 1 #increment count for all other tokens \n",
    "        return count \n",
    "        \n",
    "    def calc_perplexity(self, sum_log_prob, N):\n",
    "        #sum_log_prob = sum of all ngrams prob (in log) \n",
    "        #N = self.token_count # N = total number of tokens in input \n",
    "        return math.exp(-1 * sum_log_prob / N)       \n",
    "    \n",
    "    # Good-Turing Smoothing implemented here \n",
    "    # N = total # number of bigrams in input \n",
    "    def calc_bigram_prob(self, N):\n",
    "        # 1. make a frequency of frequency of bigrams table\n",
    "        freq_of_freq = {} \n",
    "        for bigram, count in self.bigram_freq.items():\n",
    "            if count in freq_of_freq: freq_of_freq[count] = freq_of_freq[count] + 1\n",
    "            else: freq_of_freq[count] = 1 \n",
    "        max_freq_count = max(list(freq_of_freq.keys()))\n",
    "        # 2. calculate GT probability of each bigram \n",
    "        bigram_prob_dict = {} \n",
    "        prob = 0 \n",
    "        prob_sum = 0 # to be used for renormalization \n",
    "        for bigram in self.bigram_freq.keys():\n",
    "            bigram_count = self.bigram_freq[bigram]\n",
    "            if bigram_count <= 1: #if not observed \n",
    "                prob = freq_of_freq[1] / N\n",
    "            elif 1 < bigram_count <= 10: #if observed between 1 and r times \n",
    "                prob = (bigram_count + 1)*(freq_of_freq[bigram_count +1] / freq_of_freq[bigram_count])\n",
    "            elif 10 < bigram_count <= max_freq_count: #if bigram count is the max freq count --> calc mle \n",
    "                unigram = bigram[0]\n",
    "                prob = self.calc_mle(bigram_count, self.unigram_freq[unigram]) \n",
    "                \n",
    "            prob_sum = prob_sum + prob + bigram_count \n",
    "            bigram_prob_dict[bigram] = prob \n",
    "        \n",
    "        # 3. renormalize probabilities to add to 1 \n",
    "        for bigram, prob in bigram_prob_dict.items():\n",
    "            bigram_prob_dict[bigram] = prob / prob_sum \n",
    "    \n",
    "        return bigram_prob_dict \n",
    "         \n",
    "    def train(self, input_file, UNK_threshold = 5):\n",
    "        print(\"----Training Model----\")\n",
    "        input_text = self.load_input(input_file)\n",
    "        self.tokens = self.preprocess_training(input_text, UNK_threshold)\n",
    "        self.unigram_freq = self.calc_ngram_freq(self.tokens, keys = self.tokens)\n",
    "        bigram_matrix = self.generate_bigram_matrix() # generates all possible bigrams given the vocabulary \n",
    "        bigrams = self.generate_ngrams(2, self.tokens) # generates all bigrams that occur in training text\n",
    "        self.bigram_freq = self.calc_ngram_freq(bigrams, bigram_matrix)\n",
    "        self.bigram_prob = self.calc_bigram_prob(N = len(bigrams))\n",
    "        return \n",
    "        \n",
    "    def test(self, test_file):\n",
    "        print(\"----Testing Model----\\n\")\n",
    "        test = self.load_input(test_file)\n",
    "        test = self.clean_string(test)\n",
    "        test_lines = str.splitlines(test)\n",
    "        perplexity_list = []\n",
    "        for line in test_lines:\n",
    "            test_tokens = self.tokenize(line)\n",
    "            for index, token in enumerate(test_tokens):\n",
    "                if token in self.unigram_freq: continue\n",
    "                else: test_tokens[index] = \"<UNK>\"\n",
    "            sum_log_prob = 0\n",
    "            test_bigrams = self.generate_ngrams(2, tokens = test_tokens)\n",
    "            \n",
    "            for test_bigram in test_bigrams: \n",
    "                unigram_prob = self.unigram_freq[test_bigram[0]] / len(self.tokens)\n",
    "                bigram_prob = self.bigram_prob[test_bigram] / unigram_prob #conditional probability of each bigram\n",
    "                sum_log_prob = sum_log_prob + math.log(bigram_prob)\n",
    "                \n",
    "            perplexity = self.calc_perplexity(sum_log_prob, self.token_count(test_tokens))\n",
    "            perplexity_list.append(perplexity)\n",
    "        return perplexity_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_path(file_name):\n",
    "    absolutepath = os.path.abspath('')\n",
    "    #print(absolutepath)\n",
    "    fileDirectory = os.path.dirname(absolutepath)\n",
    "    file_path = os.path.join(fileDirectory, 'Data/' + str(file_name))   \n",
    "    #print(file_path)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train = get_rel_path(\"Input/LangId.train.English\")\n",
    "french_train = get_rel_path(\"Input/LangId.train.French\")\n",
    "italian_train = get_rel_path(\"Input/LangId.train.Italian\")\n",
    "training_files = [english_train, french_train, italian_train]\n",
    "\n",
    "test_file = get_rel_path(\"Validation/LangId.test\")\n",
    "validation_file = get_rel_path(\"Validation/labels.sol\")\n",
    "f = open(validation_file, \"r\")\n",
    "validation = f.read()\n",
    "f.close()\n",
    "validation = str.splitlines(validation)\n",
    "expected_results = []\n",
    "for line in validation:\n",
    "    line = line.split()\n",
    "    expected_results.append(line[-1])\n",
    "\n",
    "def train_and_evaluate(training_files, test_file, expected_results, UNK_threshold = 5):\n",
    "    # 1. train 3 models on each language \n",
    "    models_perplexities = [] \n",
    "    for i in range(len(training_files)):\n",
    "        model = Word_Bigram_Model()\n",
    "        model.train(training_files[i], UNK_threshold = UNK_threshold) \n",
    "        perplexities = model.test(test_file) # 2. test model & obtain perplexities for each sentence \n",
    "        models_perplexities.append(perplexities)\n",
    "        \n",
    "    \n",
    "    # 3. Compare perplexities of each sentence/model & assign a language \n",
    "    test_results = [] \n",
    "    for i in range(len(models_perplexities[0])):\n",
    "        e = models_perplexities[0][i] #pull perplexities of each models for specific sentence\n",
    "        f = models_perplexities[1][i]\n",
    "        i = models_perplexities[2][i]\n",
    "        sort = sorted([e, f, i])[:3] #sorts from least to greatest ; lower perplexity --> higher probability \n",
    "        if sort[0] == e: test_results.append(\"English\")\n",
    "        if sort[0] == f: test_results.append(\"French\")\n",
    "        if sort[0] == i: test_results.append(\"Italian\")\n",
    "    \n",
    "    # 4. Compare observed results with expected results & return % accurate \n",
    "    correct_count = 0 \n",
    "    for i in range(len(expected_results)):\n",
    "        if expected_results[i] == test_results[i]: \n",
    "            correct_count = correct_count + 1\n",
    "    accuracy = (correct_count / len(expected_results)) * 100\n",
    "    print(\"Accuracy of Word Bigram Model: \", accuracy, \"%\")\n",
    "    return test_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training Model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training Model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training Model----\n",
      "----Testing Model----\n",
      "\n",
      "Accuracy of Word Bigram Model:  98.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "#test_results = train_and_evaluate(training_files, test_file, expected_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training Model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training Model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training Model----\n",
      "----Testing Model----\n",
      "\n",
      "Accuracy of Word Bigram Model:  99.0 %\n"
     ]
    }
   ],
   "source": [
    "test_results = train_and_evaluate(training_files, test_file, expected_results, UNK_threshold = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to output file \n",
    "output_file = get_rel_path(\"Output/wordLangId2.out\")\n",
    "f = open(output_file, \"w\")\n",
    "for index, result in enumerate(test_results):\n",
    "    f.write(str(index + 1) + \" \" + str(result) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
