{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Name: Zeynep Marasli\n",
    "\n",
    "Algorithm Outline:\n",
    "1. Preprocess text:\n",
    "    - all lowercase; remove numerical values\n",
    "    - remove whitespace before terminal punctuation marks \n",
    "    - tokenize (character)\n",
    "    - add beginning/end sentence markers <s>, </s>\n",
    "2. Identify rare tokens in trainining set based on freq threshold \n",
    "3. Replace all rare tokens in training set with <UNK>\n",
    "4. Create new vocabulary V’: all tokens in modified training set (V’ = {all \n",
    "   common words, <UNK>, <s>, </s>})\n",
    "5. Generate unigram matrix frequency table\n",
    "6. Generate bigram matrix table (all possible bigrams from V') \n",
    "7. Count frequencies of occurring bigrams \n",
    "8. Normalize & compute MLE values\n",
    "    - Laplace smoothing applied here \n",
    "9. In test string, replace all unseen tokens with <UNK> \n",
    "10. Compute probability/perplexity of test string  \n",
    "11. Return perplexities of each test string as list\n",
    "12. Compare each of the 3 language models' perpelxities for each test string \n",
    "    ex. If English model has lowest perplexity of a sentence, then decide test string is English, etc. \n",
    "13. Write test results to text file\n",
    "14. Assess accuracy of model --> % correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import math\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Letter_Bigram_Model():\n",
    "    def __init__(self):\n",
    "        self.tokens = [] #the preprocssed/tokenized/UNK treated training set  \n",
    "        self.bigram_freq = {}\n",
    "        self.unigram_freq = {}\n",
    "        self.bigram_prob = {}\n",
    "    \n",
    "    def load_input(self, file):\n",
    "        f = open(file, \"r\")\n",
    "        text = f.read()\n",
    "        f.close()\n",
    "        return text \n",
    "    \n",
    "    # helper function: called in preprocess()\n",
    "    # takes in a string & returns if tokenized by character\n",
    "    # adds beginning/end sentence markers \n",
    "    def tokenize(self, string):\n",
    "        tokens_list = []\n",
    "        lines = str.splitlines(string)\n",
    "        for line in lines:\n",
    "            tokens_list.append(\"<s>\") #add beginning sentence marker first    \n",
    "            for c in line: tokens_list.append(c) #for any other character\n",
    "            tokens_list.pop() #remove extra space \n",
    "            tokens_list.append(\"</s>\")\n",
    "        #print(tokens_list)\n",
    "        return tokens_list \n",
    "    \n",
    "    # helper function: called in UNK_treated_input()\n",
    "    # replaces a given item in tokens_list with UNK based on frequency \n",
    "    def insert_UNK(self, item_to_replace, tokens_list):\n",
    "        for index, token in enumerate(tokens_list):\n",
    "            if token == item_to_replace: tokens_list[index] = \"<UNK>\"\n",
    "        return tokens_list\n",
    "\n",
    "    # helper function: called in preprocess()\n",
    "    # returns UNK modified training set --> all rare tokens are replaced with <UNK>\n",
    "    def UNK_treated_input(self, tokens_list, UNK_threshold = 5):\n",
    "        freq_dict = self.calc_ngram_freq(tokens_list, keys = tokens_list)\n",
    "        for key, value in freq_dict.items(): #replace rare terms in training set based on frequency \n",
    "            if value <= UNK_threshold: tokens_list = self.insert_UNK(key, tokens_list) \n",
    "        return tokens_list\n",
    "\n",
    "    # will perform various regex sub steps (ex. remove all numeric characters)\n",
    "    # will tokenize text & add beginning & end sentence markers \n",
    "    # will replace low frequency words with UNK token \n",
    "    def preprocess_training(self, string, UNK_threshold = 5):\n",
    "        string = self.clean_string(string)\n",
    "        tokens_list = self.tokenize(string)\n",
    "        tokens_list = self.UNK_treated_input(tokens_list, UNK_threshold)\n",
    "        return tokens_list\n",
    "    \n",
    "    def clean_string(self, text_string):\n",
    "        text_string = text_string.lower()\n",
    "        text_string = re.sub(r\"\\d+\", \"\", text_string) #remove numerical characters\n",
    "        text_string = re.sub(r\"[$&%#-\\*]\", \"\", text_string)\n",
    "        #text_string = re.sub(r'[àßêíèäöéüáã]', \"\", text_string) #remove non-english characters from english text \n",
    "        text_string = re.sub(\"  +\", \" \", text_string) #remove multiple spaces\n",
    "        text_string = re.sub(r\"\\s([?.!:;\\,\\'])\", r'\\1', text_string) #remove space before punctuation\n",
    "        return text_string\n",
    "     \n",
    "    def generate_ngrams(self, n, tokens):\n",
    "        ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "        ngrams = list(ngrams)\n",
    "        for ngram in ngrams: ngram = ''.join(ngram)\n",
    "        #print(ngrams)\n",
    "        return ngrams\n",
    "    \n",
    "    # given UNK_treated vocabulary, will generate all possible bigrams & return as list \n",
    "    def generate_bigram_matrix(self):\n",
    "        bigram_matrix = [] \n",
    "        vocabulary = self.get_vocabulary() \n",
    "        for token_1 in vocabulary:\n",
    "            for token_2 in vocabulary:\n",
    "                bigram = (token_1, token_2)\n",
    "                bigram_matrix.append(bigram)\n",
    "        return bigram_matrix\n",
    "    \n",
    "    def get_vocabulary(self):\n",
    "        return list(self.unigram_freq.keys())\n",
    "    \n",
    "    def calc_ngram_freq(self, ngram_list, keys = []):\n",
    "        freq_dict = {}\n",
    "        for key in keys:\n",
    "            freq_dict[key] = 0 #initialize dict & set all values to 0         \n",
    "        for item in ngram_list:\n",
    "            if item in freq_dict: freq_dict[item] = freq_dict[item] + 1 #increment count \n",
    "            else: freq_dict[item] = 1 #create key if not in list [error catching]\n",
    "        return freq_dict\n",
    "    \n",
    "    # helper function: called in calc_bigram_prob()\n",
    "    def calc_mle(self, num, denom):\n",
    "        if denom == 0: \n",
    "            print(\"DNE, returning 0 \")\n",
    "            return 0 \n",
    "        return num/denom \n",
    "    \n",
    "    # used to calculate perplexity\n",
    "    # returns the number of tokens in a given list, exluding beginning of sentence markers \n",
    "    def token_count(self, tokens_list):\n",
    "        count = 0 \n",
    "        for token in tokens_list:\n",
    "            if token == \"<s>\": continue #do not count \n",
    "            else: count = count + 1 #increment count for all other tokens \n",
    "        return count \n",
    "\n",
    "    def calc_perplexity(self, sum_log_prob, N):\n",
    "        #sum_log_prob = sum of all ngrams prob (in log) \n",
    "        #N = self.token_count # N = total number of tokens in input \n",
    "        return math.exp(-1 * sum_log_prob / N)\n",
    "\n",
    "    def calc_bigram_prob(self, smoothing = \"None\"):\n",
    "        mle = 0 \n",
    "        self.sum_log_mle = 0 # for calculating perplexity \n",
    "        self.bigram_prob = {}\n",
    "        bigram_prob_dict = {}\n",
    "        unigram = \"\"\n",
    "        for bigram, value in self.bigram_freq.items():\n",
    "            unigram = bigram[0] #get the preceding token \n",
    "\n",
    "            if smoothing == \"Laplace\": \n",
    "                num = value + 1 #increment bigram count by 1 \n",
    "                denom = self.unigram_freq[unigram] + len(self.get_vocabulary()) #augment unigram count by vocabulary size \n",
    "                mle = self.calc_mle(num, denom)\n",
    "            else: mle = self.calc_mle(value, self.unigram_freq[unigram]) #no smoothing specified \n",
    "           \n",
    "            if mle == 0:\n",
    "                #print(\"0 MLE\", bigram)\n",
    "                bigram_prob_dict[bigram] = 0\n",
    "            else: bigram_prob_dict[bigram] = math.log(mle) #store non-zero probabilities in log format \n",
    "            \n",
    "            self.sum_log_mle = self.sum_log_mle + mle \n",
    "            \n",
    "        self.perplexity = self.calc_perplexity(self.sum_log_mle, len(self.tokens))\n",
    "        #print(\"\\tPerplexity:\", self.perplexity)\n",
    "        #print(\"\\tTotal Probability:\", math.exp(self.sum_log_mle))\n",
    "        return bigram_prob_dict\n",
    "    \n",
    "    def train(self, input_file, UNK_threshold = 5, smoothing = \"None\"):\n",
    "        print(\"----Training model----\")\n",
    "        input_text = self.load_input(input_file)\n",
    "        self.tokens = self.preprocess_training(input_text, UNK_threshold)\n",
    "        self.unigram_freq = self.calc_ngram_freq(self.tokens, keys = self.tokens) \n",
    "        bigram_matrix = self.generate_bigram_matrix() #generates all the possible bigrams given the vocabulary\n",
    "        bigrams = self.generate_ngrams(2, self.tokens) #generates all occurring bigrams from the tokenized text \n",
    "        self.bigram_freq = self.calc_ngram_freq(bigrams, bigram_matrix)\n",
    "        self.bigram_prob = self.calc_bigram_prob(smoothing = smoothing)\n",
    "        return\n",
    "    \n",
    "    def test(self, test_file):\n",
    "        print(\"----Testing Model----\\n\")\n",
    "        test_text = self.load_input(test_file)\n",
    "        test_text = self.clean_string(test_text)\n",
    "        test_lines = str.splitlines(test_text)\n",
    "        perplexity_list = [] #runnuing list of model perplexity for each sentence \n",
    "        for line in test_lines: #for each line/sentence of the test file \n",
    "            test_tokens = self.tokenize(line) \n",
    "            for index, token in enumerate(test_tokens):\n",
    "                if token in self.unigram_freq: continue # unigram_freq dict contains all unique tokens \n",
    "                else: test_tokens[index] = \"<UNK>\" # replace OOV tokens with <UNK>\n",
    "            sum_log_mle = 0\n",
    "            test_bigrams = self.generate_ngrams(2, tokens = test_tokens)    \n",
    "            for test_bigram in test_bigrams: sum_log_mle = sum_log_mle + self.bigram_prob[test_bigram]\n",
    "            perplexity = self.calc_perplexity(sum_log_mle, self.token_count(test_tokens))\n",
    "            perplexity_list.append(perplexity)\n",
    "        return perplexity_list \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rel_path(file_name):\n",
    "    absolutepath = os.path.abspath('')\n",
    "    #print(absolutepath)\n",
    "    fileDirectory = os.path.dirname(absolutepath)\n",
    "    file_path = os.path.join(fileDirectory, 'Data/' + str(file_name))   \n",
    "    #print(file_path)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train = get_rel_path(\"Input/LangId.train.English\")\n",
    "french_train = get_rel_path(\"Input/LangId.train.French\")\n",
    "italian_train = get_rel_path(\"Input/LangId.train.Italian\")\n",
    "training_files = [english_train, french_train, italian_train]\n",
    "\n",
    "test_file = get_rel_path(\"Validation/LangId.test\")\n",
    "validation_file = get_rel_path(\"Validation/labels.sol\")\n",
    "\n",
    "f = open(validation_file, \"r\")\n",
    "validation = f.read()\n",
    "f.close()\n",
    "validation = str.splitlines(validation)\n",
    "expected_results = []\n",
    "for line in validation:\n",
    "    line = line.split()\n",
    "    expected_results.append(line[-1])\n",
    "    \n",
    "def train_and_evaluate(training_files, test_file, expected_results, smoothing = \"None\", UNK_threshold = 5):\n",
    "    # 1. train 3 models on each language \n",
    "    models_perplexities = [] \n",
    "    for i in range(len(training_files)):\n",
    "        model = Letter_Bigram_Model()\n",
    "        model.train(training_files[i], smoothing = smoothing, UNK_threshold = UNK_threshold) \n",
    "        perplexities = model.test(test_file) # 2. test model & obtain perplexities for each sentence \n",
    "        models_perplexities.append(perplexities)\n",
    "        \n",
    "    \n",
    "    # 3. Compare perplexities of each sentence/model & assign a language \n",
    "    test_results = [] \n",
    "    for i in range(len(models_perplexities[0])):\n",
    "        e = models_perplexities[0][i] #pull perplexities of each models for specific sentence\n",
    "        f = models_perplexities[1][i]\n",
    "        i = models_perplexities[2][i]\n",
    "        sort = sorted([e, f, i])[:3] #sorts from least to greatest ; lower perplexity --> higher probability \n",
    "        if sort[0] == e: test_results.append(\"English\")\n",
    "        if sort[0] == f: test_results.append(\"French\")\n",
    "        if sort[0] == i: test_results.append(\"Italian\")\n",
    "    \n",
    "    # 4. Compare observed results with expected results & return % accurate \n",
    "    correct_count = 0 \n",
    "    #print(\" Actual // Expected\")\n",
    "    for i in range(len(expected_results)):\n",
    "        if expected_results[i] == test_results[i]: \n",
    "            #print(i, \" \", test_results[i], \"//\", expected_results[i])\n",
    "            correct_count = correct_count + 1\n",
    "        #else: print(i, \" \", test_results[i], \"//\", expected_results[i], \" INCORRECT\")\n",
    "    accuracy = (correct_count / len(expected_results)) * 100\n",
    "    print(\"Accuracy of Letter Bigram Model: \", accuracy, \"%\")\n",
    "    return test_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training model----\n",
      "----Testing Model----\n",
      "\n",
      "Accuracy of Letter Bigram Model:  96.0 %\n"
     ]
    }
   ],
   "source": [
    "#test_results = train_and_evaluate(training_files, test_file, expected_results, smoothing = \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Training model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training model----\n",
      "----Testing Model----\n",
      "\n",
      "----Training model----\n",
      "----Testing Model----\n",
      "\n",
      "Accuracy of Letter Bigram Model:  99.0 %\n"
     ]
    }
   ],
   "source": [
    "test_results = train_and_evaluate(training_files, test_file, expected_results, smoothing = \"Laplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to output file \n",
    "output_file = get_rel_path(\"Output/letterLangId.out\")\n",
    "f = open(output_file, \"w\")\n",
    "for index, result in enumerate(test_results):\n",
    "    f.write(str(index + 1) + \" \" + str(result) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
